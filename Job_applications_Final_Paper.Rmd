---
title: "Team Global Summary Paper"
authors: "Sajan Kumar Kar, Neelima Puthanveetil, Sandhya Karki, Sai Pavan Mekala"
date: "2023-10-31"
output:
  html_document:
    code_folding: hide
    number_sections: false
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = F)
```


## 1. Introduction 

In today’s highly competitive job market, organizations consistently strive to optimize their recruitment processes to discern the most qualified candidates for diverse employment opportunities. Conversely, job seekers are engaged in augmenting their skill sets, recognizing the pivotal role this plays in securing employment. Understanding the factors that boost employability and having an intricate idea of the know-how benefits both parties. The challenge lies in the diverse factors affecting the job market like age, education attainment, and technical proficiencies, among others. 

## 2. Rationale for Topic Selection 

Given the fluctuating nature of today's job market, both employers and job seekers face unique challenges. Recognizing the factors that affect employability and making data-driven decisions can bridge the gap between talent and opportunity. Numerous studies have addressed employability, yet gaps remain. This paper aims to build on the existing knowledge by providing a fresh perspective using a novel dataset.

## 3. Dataset Overview 


The dataset titled "Employability Classification of Over 70,000 Job Applicants" was sourced from Stack Overflow and available on Kaggle. It comprises approximately 73,000 records, providing insights into various attributes such as age, education, technical proficiencies, and more. This dataset serves as a valuable resource for understanding employability dynamics in the technical and professional development fields. Fields in the original dataset include: 

* **Id**: A general Id column 

* **Age**: age of the applicant, >35 years old or <35 years old (categorical) 

* **Accessibility**: No information (categorical) 

* **EdLevel**: education level of the applicant (Undergraduate, Master, PhD…) (categorical) 

* **Employment**: whether the applicant was employed (categorical) 

* **MentalHealth**: whether the applicant had mental health issues (categorical) 

* **Gender**: gender of the applicant, (Man, Woman, or NonBinary) (categorical) 

* **MainBranch**: whether the applicant is a profesional developer (categorical) 

* **YearsCode**: how long the applicant has been coding (integer) 

* **YearsCodePro**: how long the applicant has been coding in a professional context, (integer) 

* **Country**: Country of the applicant  

* **PreviousSalary**: the applicant's previous job salary (float) 

* **HaveWorkedWith**: skills the applicant has worked with. 

* **ComputerSkills**: number of computer skills known by the applicant (integer) 

* **Employed**: target variable, whether the applicant has been hired (categorical) 

Here’s a view of how the dataset looks like:

```{r data}
library(ezids)
df = data.frame(read.csv('stackoverflow_full.csv', header = TRUE))
xkabledplyhead(df, title = "Job Applicants Data")
```


## 4. SMART Questions: 

Our SMART questions arose from our preliminary understanding of the job market's challenges: 

1. Is there a significant difference between employed males and non-males in their respective education levels? 

2. Does mental health influence previous salary? 

3. Can we predict with at least 60% accuracy whether an individual will be employed or not, based on just their age, education level and number skills?

4. Is there a notable difference between distance based models and tree based models?



## 5. Exploratory Data Analysis (EDA) 

### 5.1. Data exploration
The dataset is found to have 73462 observations with no missing values or duplicates in any columns. It consists of some numerical and some categorical columns.

The numerical columns are: 
```{r numeric columns}
# Extract the numeric columns
colnames(subset(df, select = c(names(df)[sapply(df, is.numeric)])))
```
The categorical columns are:
```{r char columns}
# Extract the character columns
colnames(subset(df, select = c(names(df)[sapply(df, is.character)])))

```
The categorical variables are of type character in the dataset. There are 172 countries and the skills in `HaveWorkedWith` column are mixed together.

### 5.2. Data Cleaning

In the context of this analysis, 
* The `Index` column has been deemed unnecessary and subsequently removed. 
* For enhanced readability, the categorical values within the `Employment` and `Employed` columns have been transformed into more interpretable representations. 
* All character columns, with the exception of `Country` and `HaveWorkedWith` have been recast as factor variables. 
* The `EdLevel` variable, indicative of education level, is treated as an ordinal variable. Notably, the value 'other' has been retained as the lowest ordinal rank due to a lack of specific information. In an ideal scenario, it would be omitted from the ordinal ranking of education levels.


```{r preprocessing, echo=FALSE}
df_clean <- subset(df, select = -c(Index))
df_clean$Employment = as.character(df_clean$Employment)
df_clean$Employment[df_clean$Employment == "1"] <- "currently_employed"
df_clean$Employment[df_clean$Employment=="0"] <- "not_currently_employed"
df_clean$Employment = factor(df_clean$Employment)
df_clean$Employed = as.character(df_clean$Employed)
df_clean$Employed[df_clean$Employed == "1"] <- "hired"
df_clean$Employed[df_clean$Employed == "0"] <- "not_hired"
df_clean$Employed = factor(df_clean$Employed)
df_clean$Age = factor(df_clean$Age)
df_clean$Accessibility = factor(df_clean$Accessibility)
df_clean$EdLevel = factor(df_clean$EdLevel, ordered = T, 
                          levels = c("Other", "NoHigherEd", "Undergraduate", "Master", "PhD"))
df_clean$Gender = factor(df_clean$Gender)
df_clean$MentalHealth = factor(df_clean$MentalHealth)
df_clean$MainBranch = factor(df_clean$MainBranch)


```

It was checked if there was data where their professional coding experience was more than their coding experience. 588 such records were found, which had to be removed because of them clearly being faulty data.
```{r remove faulty data}
df_clean = subset(df_clean, subset = df_clean$YearsCode>df_clean$YearsCodePro)
```

Subsequently, boxplots and histograms were employed to scrutinize the dataset for potential outliers and to assess the distribution characteristics of the variables.

```{r outlier detection, echo=FALSE, warning=FALSE}
library(ggplot2)
library(tidyr)

# Create a list of the columns that we use in combined boxplot
columns_to_plot <- c("YearsCode", "YearsCodePro", "PreviousSalary", "ComputerSkills")

# Set up the plotting area to display all boxplots together
par(mfrow = c(2, length(columns_to_plot)))

# Display the boxplots
for (i in 1:length(columns_to_plot)) {
  boxplot(df_clean[[columns_to_plot[i]]], main = columns_to_plot[i],col = "lightblue")
}
for (i in 1:length(columns_to_plot)) {
  hist(df_clean[[columns_to_plot[i]]], main = columns_to_plot[i], xlab = element_blank(), col = "lightblue")
}
# plotting area layout
par(mfrow = c(2, 1))

```

Evident were right-skewed distributions, accompanied by a notable presence of outlier data points. All the outliers were on the higher end. The detection and treatment of outliers involved the examination of values exceeding $Q3 + 1.5(IQR)$,  guided by domain-specific knowledge.

So, as per the IQR method, for it to be an outlier we got:

* 39.5 years of coding experience
* 25.5 years of professional coding experience
* 193596 as the previous salary
* 30.5 as the number of computer skills

Someone could have 25.5+ years of professional coding experience. So, it didn't make sense to remove everything above this value. A good approximation would be 40 years. So we considered anyone who has above 40 years of coding and professional coding experience as an outlier and thus removed such observations.

31 computer skills is quite a lot for some, not so much for others. So we went with a higher approximation of 35 to remove outliers for this.

193k seemed a good critical value for an outlier for previous salary, but since we are ignorant of the currency of salary, we did not remove the higher salary values. There could be people who had an extremely high salary. This scenario isn't unlikely in our world.

```{r outlier removal, echo=FALSE}
df_clean = subset(df_clean, subset = df_clean$YearsCode>df_clean$YearsCodePro)
df_clean = subset(df_clean, subset = df_clean$YearsCode < 40)
df_clean = subset(df_clean, subset = df_clean$YearsCodePro < 40)
df_clean = subset(df_clean, subset = df_clean$ComputerSkills < 35)

df_final <- df_clean
# print(paste("Number of records in final data:", nrow(df_final)))
```
The final cleaned data has `r nrow(df_final)` observations.

### 5.3. Visualizations

Various visualizations were implemented to get a better look at the data and to get answers for the questions that we proposed.

We observed that most applicants who are currently employed had undergraduate or master's level of education.

```{r plot- Ed by Emp}
ggplot(data = df_final, aes(EdLevel)) + 
  geom_bar(aes(fill = Employment), position = 'dodge', alpha = 0.5) +
  labs(title = "Education Level by Employment", x="Education Level", y="Count")

```

Going a step further, for all the employed workers, we see that majority are men who have undergraduate level education. Applicants with master's level of education come in second but there's a clear difference between the two. For genders 'NonBinary' and 'Woman' there are extremely few applicants comapred to 'Man'.

```{r plot- Ed by Gender, echo=FALSE}

ggplot(data = subset(df_final, subset = Employed=='hired'), aes(EdLevel, after_stat(count))) + 
  geom_bar(aes(fill = Gender), position = 'dodge', alpha = 0.5) +
  labs(title = "Education Level by gender for employed workers", x="Education Level", y="Count")

```


On checking the previous salary of the applicant based on their mental health, interestingly, we found that those with mental health issues fall in the higher ranges of salary and those with no mental health issues fall in the lower ranges of salary.

```{r plot- PrevSal by MentalHealth(1)}
ggplot(data = df_final, aes(x = PreviousSalary, fill = MentalHealth)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density plot of Previous Salary by Mental Health", x="Previous Salary", y= 'Density')

```

## 6. Hypothesis Testing

### Test 1: Is there a significant difference between employed males and non-males in their respective education levels?

We first want to check whether there is a difference in the number of males vs non-males employed at every level of education.

Because we have over 65,000 data points, we may use statistical methods that apply to data satisfying normality conditions (because of the central limit theorem).

Hence, we use a series of two sample z tests to conduct our first set hypothesis tests.

The assumptions we need to satisfy are:\
- Large sample size\
- Data are collected as a simple random sample (so that observations are independent from one another)

We first want to look at the individuals with a bachelor's degree.

$H_0:$ There is no difference in the number of undergraduate males who are employed when compared to nonmales\
$H_a:$ There is a significant difference in the number of undergraduate males who are employed when compared to nonmales
```{r, two sample z test for proportions: undergraduate}
# We first subset into the different education levels, here undergraduate
undergrad <- subset(df_clean, df_clean$EdLevel == "Undergraduate")


# For the two sample z test, we need our two samples, males and nonmales
undergrad_male <- subset(undergrad, undergrad$Gender == "Man") # Sample 1, males

undergrad_male_employed <- subset(undergrad_male, undergrad_male$Employment == "currently_employed")

undergrad_nonmales <- subset(undergrad, undergrad$Gender != "Man") # Sample 2, females and nonbinary

undergrad_nonmales_employed <- (subset(undergrad_nonmales, undergrad_nonmales$Employment == "currently_employed"))


undergrad_employed_ztest <- prop.test(c(nrow(undergrad_male_employed), nrow(undergrad_nonmales_employed)), c(nrow(undergrad_male),nrow(undergrad_nonmales)))
undergrad_employed_ztest
```
The proportion of undergraduate males who are employed is: `r nrow(undergrad_male_employed)`/`r nrow(undergrad_male)`\
The proportion of undergraduate nonmales who are employed is: `r nrow(undergrad_nonmales_employed)`/`r nrow(undergrad_nonmales)`\
Our two sample z test statistic is: `r undergrad_employed_ztest$statistic` which has an associated p-value of `r undergrad_employed_ztest$p.value`.\
At and alpha-level of .05 we fail to reject the null hypothesis and conclude that there is not a significant difference in the number of males vs nonmales who have a bachelor's degree and are employed.

We now look at individuals with a masters.

$H_0:$ There is no difference in the number of males with a master's degree who are employed when compared to nonmales\
$H_a:$ There is a significant difference in the number of males with a master's degree who are employed when compared to nonmales

```{r, two sample z test for proportions: masters}
# We first subset into the different education levels, here masters
masters <- subset(df_clean, df_clean$EdLevel == "Master")


# For the two sample z test, we need our two samples, males and nonmales
masters_male <- subset(masters, masters$Gender == "Man") # Sample 1, males

masters_male_employed <- subset(masters_male, masters_male$Employment == "currently_employed")


masters_nonmales <- subset(masters, undergrad$Gender != "Man") # Sample 2, females and nonbinary

masters_nonmales_employed <- (subset(masters_nonmales, masters_nonmales$Employment == "currently_employed"))


masters_employed_ztest <- prop.test(c(nrow(masters_male_employed), nrow(masters_nonmales_employed)), c(nrow(masters_male),nrow(masters_nonmales)))
masters_employed_ztest

```
The proportion of males with a master's who are employed is: `r nrow(masters_male_employed)`/`r nrow(masters_male)`\
The proportion of nonmales with a master's who are employed is: `r nrow(masters_nonmales_employed)`/`r nrow(masters_nonmales)`\
Our two sample z test statistic is: `r masters_employed_ztest$statistic` which has an associated p-value of `r masters_employed_ztest$p.value`.\
At and alpha-level of .05 we reject the null hypothesis and conclude that there is a significant difference in the number of males vs nonmales who have a master's degree and are employed.

Finally, we examine individuals who have a doctorate degree.

$H_0:$ There is no difference in the number of males with a PhD who are employed when compared to nonmales\
$H_a:$ There is a significant difference in the number of males with a PhDwho are employed when compared to nonmales
```{r, two sample z test for proportions: Phd}
# We first subset into the different education levels, here PhD
phd <- subset(df_clean, df_clean$EdLevel == "PhD")


# For the two sample z test, we need our two samples, males and nonmales
phd_male <- subset(phd, phd$Gender == "Man") # Sample 1, males

phd_male_employed <- subset(phd_male, phd_male$Employment == "currently_employed")

phd_nonmales <- subset(phd, phd$Gender != "Man") # Sample 2, females and nonbinary

phd_nonmales_employed <- (subset(phd_nonmales, phd_nonmales$Employment == "currently_employed"))


phd_employed_ztest <- prop.test(c(nrow(phd_male_employed), nrow(phd_nonmales_employed)), c(nrow(phd_male),nrow(phd_nonmales)))
phd_employed_ztest
```
The proportion of males with a master's who are employed is: `r nrow(phd_male_employed)`/`r nrow(phd_male)`\
The proportion of nonmales with a master's who are employed is: `r nrow(phd_nonmales_employed)`/`r nrow(phd_nonmales)`\
Our two sample z test statistic is: `r phd_employed_ztest$statistic` which has an associated p-value of `r phd_employed_ztest$p.value`.\
At and alpha-level of .05 we fail to reject the null hypothesis and conclude that there is not a significant difference in the number of males vs nonmales who have a doctorate degree and are employed.

In real-world scenarios, these results might suggest that, in certain employment contexts, master's education appears to be a distinguishing factor between males and non-males. This could reflect societal or industry biases, or possibly differing educational or career choices among genders. Companies keen on diversity might want to address such disparities in their recruitment and mentorship programs, ensuring opportunities aren't skewed by gender-based educational trends.


### Test 2: Does mental health influence previous salary?
To answer our final question, we conduct a two sample t test to compare the mean salary between those with and without mental health issues. (We use a t test as opposed to its normal counterpart, a z test, even though we have a large sample, size since the population variance is unknown.)

The assumptions we need to satisfy are:

- The observations are independent of one another\
- The data in each group were take from a simple random sample\
- The data are relatively normally distributed
- The data is continuous in nature\
- Homogeneity of variances

$H_0:$ Whether an individual is a professional developer or not is independent of their age.\
$H_a:$ Age and status of being a professional developer are associated with each other.
```{r, two sample t-test for means}
# Subset the PreviousSalary for individuals with 'Yes' and 'No' in the MentalHealth column
salary_yes <- df_clean$PreviousSalary[df_clean$MentalHealth == "Yes"]
salary_no <- df_clean$PreviousSalary[df_clean$MentalHealth == "No"]

# Checking the mean of both groups just to get an idea
format(mean(salary_yes, na.rm = TRUE), digits=5)
format(mean(salary_no, na.rm = TRUE), digits=5)

# Now we conduct a two-sample t-test on PreviousSalary between the two groups
ttest_2sample_salary_yes_no <- t.test(salary_yes, salary_no)
ttest_2sample_salary_yes_no

```
The average salary for those who responded\
'yes' to having mental health issues: $72,709.47\
'no' to having mental health issues: $64,652.15\
Our t test statistic is `r ttest_2sample_salary_yes_no$statistic` with p-value `r ttest_2sample_salary_yes_no$p.value`. At an alpha-level of .05 we reject the null hypothesis. We conclude that the mean salary of those with mental health issues is significantly different (in this case, higher) than those without.

This shows that those acknowledging mental health issues earned between $7,141 to $8,973 more than those who didn't. This suggests that a higher salary often carries added responsibilities, leading to stress and potential mental health challenges. Companies and HR teams can use this insight to monitor employee well-being. Job seekers should also be aware that higher salaries might come with increased mental health considerations, aiding them in making informed decisions.

*The final 2 questions proposed will be answered later after model building*

## 7. Data Preprocessing

In preparation for model building, we want to check if the data types are accurate, encode categorical variables, scale any parameters if needed, perform feature engineering, and check for severe data imbalance.

```{r, data prep checking}
str(df_final)
```

In our exploratory data analysis (EDA) segment, it is observed that all variables have been appropriately transformed into their respective data types. We still leave the `Country` variable in its original format, since there are far too many variables for practical analysis based on the presently available methodologies. (Perhaps we can consider smaller analysis by subsetting the data based on "Country".) 

Proceeding furthur, we will encode our categorical variables; namely `Age`, `Accessibility`, `EdLevel`, `Employment`, `Gender`, `MentalHealth`, and `MainBranch` (leaving "Country" as it is).

In light of the ordinal nature of `Age` and `EdLevel` variables, an ordinal encoding approach is employed for the purpose of pre-processing this subset of categorical data.\
Subsequently, given that `Accessibility`, `Employment`, `MentalHealth`, and `MainBranch` are nominal variables featuring only 2 categories each, one-hot encoding them will just unnecessarily create more features, so we will label encode them.\
`Gender` has 3 categories and is nominal, so we will use the one-hot-encoding method.

```{r, encoding}
library(mltools)
library(data.table)
# saving df_final copy
df_enc = df_final
# We first want to set age as an ordinal factor:
df_enc$Age = factor(df_enc$Age, ordered = T, 
                          levels = c("<35", ">35"))
# EdLevel was already converted to an ordinal variable in the EDA section so we can proceed
df_enc$Age <- as.numeric(df_enc$Age)
df_enc$EdLevel <- as.numeric(df_enc$EdLevel)
df_enc$Accessibility <- as.numeric(df_enc$Accessibility)
df_enc$Employment <- as.numeric(df_enc$Employment)
df_enc$MentalHealth <- as.numeric(df_enc$MentalHealth)
df_enc$MainBranch <- as.numeric(df_enc$MainBranch)

one_hot_df <- subset(df_enc, select = Gender)
one_hot_df <- one_hot(as.data.table(one_hot_df))
df_before_oneHot <- df_enc

# dropping the categorical variables that we just performed one hot encoding on and adding the new columns:
drops <- "Gender"
df_enc <- df_enc[ , !(names(df_enc) %in% drops)]

df_enc <- cbind(df_enc, one_hot_df)

```

The "HaveWorkedWith" column is too disorderly, making it hard to analyze or work with.. The `ComputerSkills` feature is built on top of it so we'll be using this. The `country` column was experimented with but didn't give good results and was hence removed.

```{r column remove}
df_enc <- subset(df_enc, select = -c(HaveWorkedWith))
df_final2 <- subset(df_enc, select = -c(Country))
```

``` {r Imbalance check}
ggplot(data = df_final2, aes(Employed, after_stat(count))) + 
  geom_bar(alpha = 0.5, col = 'navy')

```

An imbalance check suggests the absence of any major imbalance, eliminating the necessity for any data resampling measures, and we can proceed normally.\
Finally, for the numerical variables in the train set, we want to scale them using standardization.

```{r standardization}
library(dplyr)
df_final2 <- df_final2 %>%
  mutate_at(vars('YearsCode', 'YearsCodePro', 'PreviousSalary', 'ComputerSkills'), scale)
```

## 8. Model Building

We first split our dataset into train and test sets for modeling purposes.

```{r train test split}
data_sample <- sample(c(TRUE, FALSE), nrow(df_final2), replace=TRUE, prob=c(0.8, 0.2))
train <- df_final2[data_sample, ]
test <- df_final2[!data_sample, ]
```

Since we do not have unbalanced data (for our target column), and we want to know the overall performance of the model, we'll use AUC ROC as our evaluation metric.

```{r Logistic Regression model}
set.seed(47)
lr_model <- glm(Employed~., data = train, family = "binomial")
summary(lr_model)

```

From the p-values, we observe that `EdLevel`, `Employment`, `MainBranch`, `PreviousSalary` and `ComputerSkills` emerge as the most important features. Let's further evaluate the model and try to validate these findings.

```{r LR- AUC ROC Evaluation}
library(pROC)
pred=predict(lr_model, test, type = "response" )
roc_lr <- roc(test$Employed, pred)
auc(roc_lr) # area-under-curve prefer 0.8 or higher.
plot(roc_lr, print.auc = T, main = "ROC Curve for LR Model", col = "#514BA2", lwd = 2)
```

We obtain an Area under the curve as `r auc(roc_lr)` which  indicates a commendable performance by the Logistic Regression model.

```{r KNN model}
train_data <- train[, names(train) != "Employed"]
test_data <- test[, names(test) != "Employed"]
train_labels <- train$Employed

# KNN model with k = 3
k <- 3
knn_model <- class::knn(train = train_data, test = test_data, cl = train_labels, k = k, prob=TRUE)
roc_knn <-roc(test$Employed, attributes(knn_model)$prob)
auc(roc_knn)
plot(roc_knn, print.auc = T, main = "ROC Curve for KNN Model", col = "#514BA2", lwd = 2)
```

We get an AUC of `r auc(roc_knn)`. The KNN Model doesn't give us a good result, unfortunately. 

```{r SVM model}
library(e1071)
svm_model <- svm(Employed ~ ., data = train, kernel = "linear", probability = TRUE)
svm_predictions <- predict(svm_model, test, probability = TRUE)
roc_svm <- roc(test$Employed, as.numeric(svm_predictions))
auc(roc_svm)
plot(roc_svm, print.auc = T, main = "ROC Curve for SVM Model", col = "#514BA2", lwd = 2)
```

`r auc(roc_svm)` is a pretty good result but our logistic regression model is still the best.

```{r Decision Tree model}
library(rpart)
library(rpart.plot)
set.seed(47)
dt_model <- rpart(Employed ~ ., data=df_final2)
plotcp(dt_model)
summary(dt_model)
printcp(dt_model)

rpart.plot(dt_model, main="Decision Tree for Employment")

dt_pred <- predict(dt_model, test, type = 'prob')# instead of p1 dt_pred
dt_pred <- dt_pred[,2]
roc_dt <- roc(test$Employed, dt_pred)# roc_dt
auc(roc_dt)
plot(roc_dt, print.auc = T, main = "ROC Curve for Decistion Tree Model", col = "#514BA2", lwd = 2)
```

It does give a good enough result of `r auc(roc_dt)`. The Logistic Regression model outperforms this but it's still a decent model. Tree models work a lot better.
Perhaps, tuning this might give us a better model.

```{r Decision Tree Tuning, message=FALSE, include=FALSE}
minsplit_vals <- c(2, 3, 5, 7, 10, 12, 15, 17, 20, 23, 27, 30, 45, 50, 75, 91, 100)
max_depth_vals <- c(2, 3, 5, 7, 10, 12, 15, 17, 20, 23, 27, 30)

auc_values = c()

# Nested for loop to iterate through all combinations
for (minsplit_val in minsplit_vals) {
  for (max_depth_val in max_depth_vals) {
    dt_model_loop <- rpart(Employed ~ ., data=train, minsplit = minsplit_val, maxdepth = max_depth_val)
    p1_loop <- predict(dt_model_loop, test, type = 'prob')
    p1_loop <- p1_loop[,2]
    r_loop <- roc(test$Employed, p1_loop, percent = TRUE)
    AUC_val <- auc(r_loop)
    auc_values <- c(auc_values, AUC_val)
    print(paste('The AUC value for model with minsplit =', minsplit_val, 'and maxdepth =', max_depth_val, 'is', round(AUC_val, digits = 2), '%'))
  }
}
max(auc_values)
```

Even with hyperparameter tuning, we obtain the same AUC ROC score `r max(auc_values)`. We also notice that the decision tree model decides just on the basis of a single feature- `ComputerSkills`. Out of curiosity, let's try a model with everything except the number of computer skills.

```{r, DT without computer skills}
dt_model_nocompskills <- rpart(Employed ~ Age + Accessibility + EdLevel + Employment + MentalHealth + MainBranch + YearsCode + YearsCodePro + PreviousSalary + Gender_Man + Gender_NonBinary + Gender_Woman, data=train)

rpart.plot(dt_model_nocompskills, main="Decision Tree for Employment")
dt_nocs_pred <- predict(dt_model_nocompskills, test, type = 'prob')
dt_nocs_pred  <- dt_nocs_pred [,2]
r <- roc(test$Employed, dt_nocs_pred )
auc(r)
plot(r, print.auc = T, main = "ROC Curve for Decistion Tree Model Without Computer Skills", col = "#514BA2", lwd = 2)
```

It uses some other features but the AUC ROC drops significantly.

Now that we have our models ready, let's go back and answer the final two proposed questions.

### 3. Can we predict with at least 60% accuracy whether an individual will be employed or not, based on just their age, education level and number of skills?

We notice that our best model is the Logistic Regression model. Let's use that to get an answer for this question.

```{r Q3 Logistic Regression}
set.seed(47)
lr_model3 <- glm(Employed~Age +  EdLevel + ComputerSkills, data = train, family = "binomial")

pred3=predict(lr_model3, test, type = "response" )
roc_lr3 <- roc(test$Employed, pred3)
auc(roc_lr3) # area-under-curve prefer 0.8 or higher.
plot(roc_lr3, print.auc = T, main = "ROC Curve for LR Model [Age +Edlevel + ComputerSkills]", col = "#514BA2", lwd = 2)
```

We observe an AUC value of 0.869, which is just a little less than our original AUC of 0.872. This means that our assumption of `Age`, `EdLevel` and `ComputerSkills` being the most important factors to get hired was true. In fact, as per our model, we could practically ignore the rest of the factors.

### 4. Is there a notable difference between distance based models and tree based models?

Based on the difference in AUC ROC score between our KNN Model (`r auc(roc_knn)`) and Decision tree model (`r auc(roc_dt)`), we can conclude that there is a notable difference between the result of distance based model*(KNN)* and tree based model*(Decision Tree)* for our data.

## 9. Limitations of the Dataset 

While the dataset is comprehensive, it predominantly focuses on the tech industry and individuals from Stack Overflow, which are mostly male audiences, and it may not be wholly representative of the broader job market. Additional information on soft skills, cultural fit, and detailed job roles might have added more depth to our analysis. The distribution of gender within the dataset displays a pronounced male predominance, with a notable imbalance in favor of males. The lack of currency information within the `PreviousSalary` variable hinders the establishment of a unified metric for measurement. Furthermore, the representation of age as a categorical variable offers limited insight into the precise age in years. 

## 10. Further Research 
For the future, we have a few main suggestions. First, we recommend doing an EEPEN analysis to look more closely at specific things like age or where someone is from. This will give us clearer ideas about those areas. It would also be helpful to find the most efficient statistical technique to deal with a category that has a large number of levels (in our case, 172 countries). As we learn more, we can improve on these models and offer a thorough analysis that is helpful to both employers and individuals actively in the job market. 

## 11. References
The Future of Jobs Report 2023. World Economic Forum. (n.d.).             https://www.weforum.org/publications/the-future-of-jobs-report-2023/digest/  \
Picciotto, R. (2023, July 7). Tech roles are still “the most in-demand,” says job market expert-but you need these skills to land them. CNBC.    https://www.cnbc.com/2023/07/07/tech-jobs-are-still-the-most-in-demand-says-employment-market-expert.html \
Tankha, Ayush. (2023). *Employability Classification of Over 70,000 Job Applicants*. StackOverflow. https://www.kaggle.com/datasets/ayushtankha/70k-job-applicants-data-human-resource/data \
Technology & IT hiring trends 2023: Robert Half. English. (2023, September 15). https://www.roberthalf.com/us/en/insights/salary-hiring-trends/demand-for-skilled-talent/tech-it \
Tafvizi, A., Avci, B., &amp; Sundararajan, M. (2022, May 24). Attributing AUC-Roc to analyze binary classifier performance. arXiv.org. https://arxiv.org/abs/2205.11781 \
Hornik, Kurt & Meyer, David & Karatzoglou, Alexandros. (2006). Support Vector Machines in R. Journal of Statistical Software. 15. 10.18637/jss.v015.i09. 